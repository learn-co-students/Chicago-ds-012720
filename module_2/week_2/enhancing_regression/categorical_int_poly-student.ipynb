{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Enhancing Regression: Categorical Preprocessing, Interactions and Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![kneeding](https://media.giphy.com/media/RpckSiHL6ZaXS/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Objectives, be able to:\n",
    "\n",
    "1. Preprocess non-numeric data:\n",
    "> - categorical: get_dummies/one-hot-encoder\n",
    "> - binary encoder\n",
    "> - ordinal: label encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "2. Feature Engineer:\n",
    "> - Interaction terms\n",
    "> - Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Scenario: car seat sales\n",
    "\n",
    "We will continue to work with our car seat data.  <br>\n",
    "Here, again are all of the features.\n",
    "\n",
    "- Sales: unit sales at each location\n",
    "- CompPrice: price charged by nearest competitor at each location\n",
    "- Income: community income level\n",
    "- Advertising: local advertising budget for company at each location\n",
    "- Population: population size in region (in thousands)\n",
    "- Price: price charged for car seat at each site\n",
    "- ShelveLoc: quality of shelving location at site (Good | Bad | Medium)\n",
    "- Age: average age of the local population\n",
    "- Education: education level at each location\n",
    "- Urban: whether the store is in an urban or rural location\n",
    "- USA: whether the store is in the US or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompPrice</th>\n",
       "      <th>Income</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>ShelveLoc</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.50</td>\n",
       "      <td>138</td>\n",
       "      <td>73</td>\n",
       "      <td>11</td>\n",
       "      <td>276</td>\n",
       "      <td>120</td>\n",
       "      <td>Bad</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.22</td>\n",
       "      <td>111</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>260</td>\n",
       "      <td>83</td>\n",
       "      <td>Good</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.06</td>\n",
       "      <td>113</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>Medium</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.40</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>466</td>\n",
       "      <td>97</td>\n",
       "      <td>Medium</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.15</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>128</td>\n",
       "      <td>Bad</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n",
       "0   9.50        138      73           11         276    120       Bad   42   \n",
       "1  11.22        111      48           16         260     83      Good   65   \n",
       "2  10.06        113      35           10         269     80    Medium   59   \n",
       "3   7.40        117     100            4         466     97    Medium   55   \n",
       "4   4.15        141      64            3         340    128       Bad   38   \n",
       "\n",
       "   Education Urban   US  \n",
       "0         17   Yes  Yes  \n",
       "1         10   Yes  Yes  \n",
       "2         12   Yes  Yes  \n",
       "3         14   Yes  Yes  \n",
       "4         13   Yes   No  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Carseats.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from continuous_prep import scale_numeric\n",
    "X = df.drop('Sales', axis=1)\n",
    "y = df['Sales']\n",
    "\n",
    "X_numeric = X.select_dtypes(exclude='object')\n",
    "\n",
    "# Preprocess numeric with custom function\n",
    "X_numeric, y = scale_numeric(X_numeric, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take a moment to look through the continuous_prep.py file.  Add comments to the file to communicate its function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate object type and match index to preprocessed y\n",
    "X_cat = X.select_dtypes(include='object')\n",
    "X_cat = X_cat.iloc[X_numeric.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Different types of categorical features\n",
    "> We have three categorical features, ShelveLoc, Urban, and US.\n",
    ">  - How would you expect to translate these categories into input that a machines are familiar with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "Urban and US are binary, Yes/No, so let's handle them first.\n",
    "From sklearn.preprocessing, we can import a module called LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Label Binarizer uses familiar syntax.\n",
    "us_lb = LabelBinarizer()\n",
    "X_cat[\"US\"] = us_lb.fit_transform(X_cat[\"US\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn. Do the same with Urban\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables and One-Hot-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's deal with ShelveLoc. We have a couple of ways we could handle it. The first way we will discuss is the poorly named dummy variables.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a dummy/one-hot-encoded variable?\n",
    "\n",
    "> We create dummy variables from categorical features with multiple categories.  Each unique category is transformed into its own column filled with 0's and 1's.  The rows with 1's indicate that the row is associated with that category.  When we fit our model, we create coefficients for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the ShelveLoc feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Bad\n",
       "1        Good\n",
       "2      Medium\n",
       "3      Medium\n",
       "4         Bad\n",
       "        ...  \n",
       "395      Good\n",
       "396    Medium\n",
       "397    Medium\n",
       "398       Bad\n",
       "399      Good\n",
       "Name: ShelveLoc, Length: 387, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat.ShelveLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bad</th>\n",
       "      <th>Good</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bad  Good  Medium\n",
       "0      1     0       0\n",
       "1      0     1       0\n",
       "2      0     0       1\n",
       "3      0     0       1\n",
       "4      1     0       0\n",
       "..   ...   ...     ...\n",
       "395    0     1       0\n",
       "396    0     0       1\n",
       "397    0     0       1\n",
       "398    1     0       0\n",
       "399    0     1       0\n",
       "\n",
       "[387 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use pandas to \"get_dummies\"\n",
    "\n",
    "pd.get_dummies(X_cat.ShelveLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Footnote_<br>\n",
    "We can also use **One Hot Encoder**. OneHotEncoder has the advantage of the .fit() method.  The encoder can then be saved and used on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for label encoder\n",
    "X_cat_for_le = X_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variable Trap\n",
    "\n",
    "![](https://media.giphy.com/media/8McNH1aXZnVyE/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Take a minute to think about the ShelveLoc dummy-variable example.  If you were to remove one column, would you lose any information?  \n",
    "\n",
    "> If we took out the \"Bad\" column, is there a way we could still figure out which records had \"Bad\" shelf location?\n",
    "\n",
    "> Discuss with your neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dummy variable trap is a problem with multicollinearity.  We can create one column from a combination of the other columns.  By creating dummies in this way, we  violate an assumption of linear regression.  \n",
    "\n",
    "- When features have high collinearity, the coefficients become difficult to interpret.  Because they are collinear, the beta coefficients will be highly influenced by the presence of the other feature.\n",
    "\n",
    "- To address this problem, we drop one column.  Get dummies has parameter \"drop_first\" to address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# use get dummies to add two columns, \"Good\" and \"Medium\"\n",
    "# to our dataframe.  Drop the original ShelveLoc column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_cat.join(pd.get_dummies(X_cat['ShelveLoc'], drop_first = True))\n",
    "X_cat.drop('ShelveLoc', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Urban</th>\n",
       "      <th>US</th>\n",
       "      <th>Good</th>\n",
       "      <th>Medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Urban  US  Good  Medium\n",
       "0      1   1     0       0\n",
       "1      1   1     1       0\n",
       "2      1   1     0       1\n",
       "3      1   1     0       1\n",
       "4      1   0     0       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674167170661751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Let's rejoin our categorical and numerical columns\n",
    "# then fit our model again.\n",
    "\n",
    "X = X_numeric.join(X_cat)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.score(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Another option would be to apply Label Encoder.  Perhaps we have prior knowledge about the relative value of Bad, Mediumm, and Good shelf locations.  Perhaps we think that Medium is twice as important as Bad, and Good is 3 times as important as Bad.  We can then do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear regression to the data encoded in this way treats the feature as a continuous independent variable.  If our domain knowledge is good, then such a fit could perform better than the dummy variable.  You can try out both ways and see the effect on the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's fit our model with the bgm encoded values and see what kind of R-squared it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8606526343461581"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bgm_encoder(element):\n",
    "    if element == 'Bad':\n",
    "        return 0\n",
    "    elif element == 'Medium':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "X_example = X_numeric.join(X_cat_for_le)\n",
    "X_example['ShelveLoc'] = X_example['ShelveLoc'].apply(bgm_encoder)\n",
    "lr.fit(X_example, y)\n",
    "lr.score(X_example, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discussion: Can you imagine a scenario when label encoding could be highly effective? Consider the domain knowledge of the model designer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Interaction Terms\n",
    "An interaction term is a feature which accounts for the join affect of two other features that is non-additive.  Non-additive is the important distinction here, since our linear model is based on additive relationships of our features: the features in the data set add together to predict the target.\n",
    "\n",
    "The plot below shows an example of a non-additive relationship: the effect of a drug vs a placebo on people who experienced different severities of stroke.  Based on the three groups of strokes, the plot shows the slopes (i.e. the change in effectiven of the drug across drug and placebo treatments)  to be equivalent between mild and moderate stroke victims, but markedly different for severe stroke victims. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/stroke_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The difference in slope in one group indicates that   survival rate and type of treatment does not change equally across groups.  In other words, the effect is not additive with regards to stroke severity.  In order to capture that relationship, we create an interaction term between stroke severity and treatment.  An interaction term is just two features multiplied together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore interaction terms, we will import the mtcars dataset and fit a vanilla linear regression model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_cars = pd.read_csv('data/mtcars.csv')\n",
    "mt_cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_cars = mt_cars.drop('model', axis=1)\n",
    "y = mt_cars['mpg']\n",
    "X = mt_cars.drop('mpg', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8690157644777647\n"
     ]
    }
   ],
   "source": [
    "lr = lr.fit(X,y)\n",
    "base_score = lr.score(X,y)\n",
    "print(base_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 25 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>3.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:03:04</td>     <th>  Log-Likelihood:    </th> <td> -69.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   161.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>   177.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   12.3034</td> <td>   18.718</td> <td>    0.657</td> <td> 0.518</td> <td>  -26.623</td> <td>   51.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>       <td>   -0.1114</td> <td>    1.045</td> <td>   -0.107</td> <td> 0.916</td> <td>   -2.285</td> <td>    2.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>      <td>    0.0133</td> <td>    0.018</td> <td>    0.747</td> <td> 0.463</td> <td>   -0.024</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>        <td>   -0.0215</td> <td>    0.022</td> <td>   -0.987</td> <td> 0.335</td> <td>   -0.067</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>      <td>    0.7871</td> <td>    1.635</td> <td>    0.481</td> <td> 0.635</td> <td>   -2.614</td> <td>    4.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>        <td>   -3.7153</td> <td>    1.894</td> <td>   -1.961</td> <td> 0.063</td> <td>   -7.655</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>qsec</th>      <td>    0.8210</td> <td>    0.731</td> <td>    1.123</td> <td> 0.274</td> <td>   -0.699</td> <td>    2.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>        <td>    0.3178</td> <td>    2.105</td> <td>    0.151</td> <td> 0.881</td> <td>   -4.059</td> <td>    4.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>        <td>    2.5202</td> <td>    2.057</td> <td>    1.225</td> <td> 0.234</td> <td>   -1.757</td> <td>    6.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>      <td>    0.6554</td> <td>    1.493</td> <td>    0.439</td> <td> 0.665</td> <td>   -2.450</td> <td>    3.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>      <td>   -0.1994</td> <td>    0.829</td> <td>   -0.241</td> <td> 0.812</td> <td>   -1.923</td> <td>    1.524</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.907</td> <th>  Durbin-Watson:     </th> <td>   1.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.385</td> <th>  Jarque-Bera (JB):  </th> <td>   1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.521</td> <th>  Prob(JB):          </th> <td>   0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.526</td> <th>  Cond. No.          </th> <td>1.22e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.22e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.869\n",
       "Model:                            OLS   Adj. R-squared:                  0.807\n",
       "Method:                 Least Squares   F-statistic:                     13.93\n",
       "Date:                Tue, 25 Feb 2020   Prob (F-statistic):           3.79e-07\n",
       "Time:                        22:03:04   Log-Likelihood:                -69.855\n",
       "No. Observations:                  32   AIC:                             161.7\n",
       "Df Residuals:                      21   BIC:                             177.8\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     12.3034     18.718      0.657      0.518     -26.623      51.229\n",
       "cyl           -0.1114      1.045     -0.107      0.916      -2.285       2.062\n",
       "disp           0.0133      0.018      0.747      0.463      -0.024       0.050\n",
       "hp            -0.0215      0.022     -0.987      0.335      -0.067       0.024\n",
       "drat           0.7871      1.635      0.481      0.635      -2.614       4.188\n",
       "wt            -3.7153      1.894     -1.961      0.063      -7.655       0.224\n",
       "qsec           0.8210      0.731      1.123      0.274      -0.699       2.341\n",
       "vs             0.3178      2.105      0.151      0.881      -4.059       4.694\n",
       "am             2.5202      2.057      1.225      0.234      -1.757       6.797\n",
       "gear           0.6554      1.493      0.439      0.665      -2.450       3.761\n",
       "carb          -0.1994      0.829     -0.241      0.812      -1.923       1.524\n",
       "==============================================================================\n",
       "Omnibus:                        1.907   Durbin-Watson:                   1.861\n",
       "Prob(Omnibus):                  0.385   Jarque-Bera (JB):                1.747\n",
       "Skew:                           0.521   Prob(JB):                        0.418\n",
       "Kurtosis:                       2.526   Cond. No.                     1.22e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.22e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X.join(y)\n",
    "formula = 'mpg ~ ' + '+'.join(X.columns)\n",
    "mod = smf.ols(formula=formula, data = data)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.94</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 25 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.44e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:05:40</td>     <th>  Log-Likelihood:    </th> <td> -65.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>   155.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   173.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   29.9764</td> <td>   18.535</td> <td>    1.617</td> <td> 0.121</td> <td>   -8.687</td> <td>   68.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl</th>       <td>   -1.7896</td> <td>    1.184</td> <td>   -1.512</td> <td> 0.146</td> <td>   -4.259</td> <td>    0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disp</th>      <td>   -0.0959</td> <td>    0.049</td> <td>   -1.958</td> <td> 0.064</td> <td>   -0.198</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th>        <td>   -0.0334</td> <td>    0.020</td> <td>   -1.641</td> <td> 0.116</td> <td>   -0.076</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drat</th>      <td>   -0.5412</td> <td>    1.585</td> <td>   -0.342</td> <td> 0.736</td> <td>   -3.847</td> <td>    2.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th>        <td>   -3.5527</td> <td>    1.718</td> <td>   -2.068</td> <td> 0.052</td> <td>   -7.136</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>qsec</th>      <td>    0.6981</td> <td>    0.664</td> <td>    1.051</td> <td> 0.306</td> <td>   -0.687</td> <td>    2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vs</th>        <td>    0.8287</td> <td>    1.919</td> <td>    0.432</td> <td> 0.670</td> <td>   -3.174</td> <td>    4.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>am</th>        <td>    0.8191</td> <td>    1.998</td> <td>    0.410</td> <td> 0.686</td> <td>   -3.348</td> <td>    4.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gear</th>      <td>    1.5545</td> <td>    1.405</td> <td>    1.106</td> <td> 0.282</td> <td>   -1.377</td> <td>    4.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carb</th>      <td>    0.1442</td> <td>    0.765</td> <td>    0.189</td> <td> 0.852</td> <td>   -1.451</td> <td>    1.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cyl_disp</th>  <td>    0.0138</td> <td>    0.006</td> <td>    2.363</td> <td> 0.028</td> <td>    0.002</td> <td>    0.026</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.577</td> <th>  Durbin-Watson:     </th> <td>   2.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.455</td> <th>  Jarque-Bera (JB):  </th> <td>   1.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.259</td> <th>  Prob(JB):          </th> <td>   0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.196</td> <th>  Cond. No.          </th> <td>8.81e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.81e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.898\n",
       "Model:                            OLS   Adj. R-squared:                  0.841\n",
       "Method:                 Least Squares   F-statistic:                     15.94\n",
       "Date:                Tue, 25 Feb 2020   Prob (F-statistic):           1.44e-07\n",
       "Time:                        22:05:40   Log-Likelihood:                -65.916\n",
       "No. Observations:                  32   AIC:                             155.8\n",
       "Df Residuals:                      20   BIC:                             173.4\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     29.9764     18.535      1.617      0.121      -8.687      68.640\n",
       "cyl           -1.7896      1.184     -1.512      0.146      -4.259       0.679\n",
       "disp          -0.0959      0.049     -1.958      0.064      -0.198       0.006\n",
       "hp            -0.0334      0.020     -1.641      0.116      -0.076       0.009\n",
       "drat          -0.5412      1.585     -0.342      0.736      -3.847       2.765\n",
       "wt            -3.5527      1.718     -2.068      0.052      -7.136       0.030\n",
       "qsec           0.6981      0.664      1.051      0.306      -0.687       2.084\n",
       "vs             0.8287      1.919      0.432      0.670      -3.174       4.832\n",
       "am             0.8191      1.998      0.410      0.686      -3.348       4.986\n",
       "gear           1.5545      1.405      1.106      0.282      -1.377       4.486\n",
       "carb           0.1442      0.765      0.189      0.852      -1.451       1.740\n",
       "cyl_disp       0.0138      0.006      2.363      0.028       0.002       0.026\n",
       "==============================================================================\n",
       "Omnibus:                        1.577   Durbin-Watson:                   2.059\n",
       "Prob(Omnibus):                  0.455   Jarque-Bera (JB):                1.219\n",
       "Skew:                           0.259   Prob(JB):                        0.544\n",
       "Kurtosis:                       2.196   Cond. No.                     8.81e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.81e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create an interaction term by simply multiplying two features together:\n",
    "X['cyl_disp'] = X['cyl'] * X['disp']\n",
    "data = X.join(y)\n",
    "formula = 'mpg ~ ' + '+'.join(X.columns)\n",
    "mod = smf.ols(formula=formula, data = data)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That had a good effect on our r_squared. \n",
    "\n",
    ">Now, it is your turn to explore the data and find the most influential interaction turn.  Hint: Try using itertools combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Another way to potentially improve our score is to account for non-linear relationships between target and feature.  Polynomial transformations, for example a quadratic relationship, can account for curved relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example the relationship between age and income shown it the plot below:\n",
    "\n",
    "![](img/age_v_income.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curved shape of these relationships will not be captured by a simple linear relationship between age and income. Income increases with age until around 50, and then starts to decrease. When seeing a curve like this, you should consider that the addition of a polynomial term will capture an otherwise overlooked relationship between feature and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the famous Boston Housing dataset to practice identifying polynomial relationships.\n",
    "\n",
    "The dataset has been imported and a basic model fit with an R^2 of ~.74.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406426641094095"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "boston = sklearn.datasets.load_boston()\n",
    "X = boston.data\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = boston.feature_names\n",
    "y = pd.DataFrame(boston.target)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to explore the relationships between targets and variables, and see whether the addition of any polynomial terms will benefit the model substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# your code here\n",
    "# hint, use pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply the PolynomialFeature method from sklearn, which creates polynomial terms of a specified degree for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9289961714593022"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = boston.data\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = boston.feature_names\n",
    "y = pd.DataFrame(boston.target)\n",
    "\n",
    "pf = PolynomialFeatures(2, include_bias=False)\n",
    "\n",
    "poly_df = pd.DataFrame(pf.fit_transform(X))\n",
    "poly_df.columns = pf.get_feature_names(X.columns)\n",
    "poly_df.head()\n",
    "\n",
    "lr.fit(poly_df, y)\n",
    "lr.score(poly_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Polynomials have diminishing returns!\n",
    "\n",
    "![polynomials](https://sc.cnbcfm.com/applications/cnbc.com/resources/files/2015/12/11/emotionandincome-01_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn polynomial feature creates a ton of features. R-2 always increases with more features.  This will lead to overfitting, which will lead to poor performance on data which has not been seen. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "enhancing_regression_sixp.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
